{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial 2 - Transfer Learning\n",
    "\n",
    "Jinhyeok Jeong\n",
    "2024-05-28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchsummary\n",
    "\n",
    "Instead of using print() function, summary() function from torchinfo package can be used to examine the detailed information of the network.\n",
    "\n",
    "<code>\n",
    "pip install torchinfo\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print output:\n",
      "simpleNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (Linear1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (Linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (Linear3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (Linear4): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "summary output:\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "simpleNN                                 --\n",
      "├─Flatten: 1-1                           --\n",
      "├─Linear: 1-2                            401,920\n",
      "├─ReLU: 1-3                              --\n",
      "├─Linear: 1-4                            262,656\n",
      "├─ReLU: 1-5                              --\n",
      "├─Linear: 1-6                            262,656\n",
      "├─ReLU: 1-7                              --\n",
      "├─Linear: 1-8                            5,130\n",
      "=================================================================\n",
      "Total params: 932,362\n",
      "Trainable params: 932,362\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "class simpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear1 = nn.Linear(28 * 28, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.Linear2 = nn.Linear(512, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.Linear3 = nn.Linear(512, 512)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.Linear4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.Linear1(x))\n",
    "        x = self.relu2(self.Linear2(x))\n",
    "        x = self.relu3(self.Linear3(x))\n",
    "        z = self.Linear4(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "model = simpleNN()\n",
    "\n",
    "print(\"print output:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\\nsummary output:\")\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary can get additional arguments such as the sahpe of input data and batch_dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "simpleNN                                 [1, 10]                   --\n",
      "├─Flatten: 1-1                           [1, 784]                  --\n",
      "├─Linear: 1-2                            [1, 512]                  401,920\n",
      "├─ReLU: 1-3                              [1, 512]                  --\n",
      "├─Linear: 1-4                            [1, 512]                  262,656\n",
      "├─ReLU: 1-5                              [1, 512]                  --\n",
      "├─Linear: 1-6                            [1, 512]                  262,656\n",
      "├─ReLU: 1-7                              [1, 512]                  --\n",
      "├─Linear: 1-8                            [1, 10]                   5,130\n",
      "==========================================================================================\n",
      "Total params: 932,362\n",
      "Trainable params: 932,362\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.93\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 3.73\n",
      "Estimated Total Size (MB): 3.75\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# if the shape of input data is provided, summary would show output shape information\n",
    "# The default value of batch_dim is 0, and batch dimension should not be included in the input data shape.\n",
    "print(summary(model, (1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision package has several pre-trained models that can be used for transfer learning\n",
    "\n",
    "(e.g., AlexNet, ConvNext, DenseNet, EfficientNet, GoogLeNet, Inception V3, ResNet, VGG, VisionTransfomer, ....)\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "save & load model and weights:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "AlexNet                                  --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Conv2d: 2-1                       23,296\n",
      "│    └─ReLU: 2-2                         --\n",
      "│    └─MaxPool2d: 2-3                    --\n",
      "│    └─Conv2d: 2-4                       307,392\n",
      "│    └─ReLU: 2-5                         --\n",
      "│    └─MaxPool2d: 2-6                    --\n",
      "│    └─Conv2d: 2-7                       663,936\n",
      "│    └─ReLU: 2-8                         --\n",
      "│    └─Conv2d: 2-9                       884,992\n",
      "│    └─ReLU: 2-10                        --\n",
      "│    └─Conv2d: 2-11                      590,080\n",
      "│    └─ReLU: 2-12                        --\n",
      "│    └─MaxPool2d: 2-13                   --\n",
      "├─AdaptiveAvgPool2d: 1-2                 --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─Dropout: 2-14                     --\n",
      "│    └─Linear: 2-15                      37,752,832\n",
      "│    └─ReLU: 2-16                        --\n",
      "│    └─Dropout: 2-17                     --\n",
      "│    └─Linear: 2-18                      16,781,312\n",
      "│    └─ReLU: 2-19                        --\n",
      "│    └─Linear: 2-20                      4,097,000\n",
      "=================================================================\n",
      "Total params: 61,100,840\n",
      "Trainable params: 61,100,840\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# for some other models, there could be more than one pre-trinaed weights that are available\n",
    "# CAUTION: default value of an argument 'weights' is None (no pre-training)\n",
    "\n",
    "alexnet = torchvision.models.alexnet(\n",
    "    weights=\"DEFAULT\"\n",
    ")  # for alexnet, DEFAULT = IMAGENET1K_V1\n",
    "# resnet18 = torchvision.models.resnet18(weights='DEFAULT') # for resnet18, DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "print(summary(alexnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are various ways to see the layers of a model. children() and module() will return a generator making a list of layers in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.children of AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the network for transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During transfer learning, you may want to freeze the weights for the earlier layers.\n",
    "Freezing weights can be done by making gradients not computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pre-trained weights are freezed, final classification layer can be modified for transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.classifier[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.classifier[-1].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_features = alexnet.classifier[-1].in_features\n",
    "n_newclass = 10\n",
    "\n",
    "alexnet.classifier[-1] = nn.Linear(n_in_features, n_newclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen weights are indicated by parenthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "AlexNet                                  --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Conv2d: 2-1                       (23,296)\n",
      "│    └─ReLU: 2-2                         --\n",
      "│    └─MaxPool2d: 2-3                    --\n",
      "│    └─Conv2d: 2-4                       (307,392)\n",
      "│    └─ReLU: 2-5                         --\n",
      "│    └─MaxPool2d: 2-6                    --\n",
      "│    └─Conv2d: 2-7                       (663,936)\n",
      "│    └─ReLU: 2-8                         --\n",
      "│    └─Conv2d: 2-9                       (884,992)\n",
      "│    └─ReLU: 2-10                        --\n",
      "│    └─Conv2d: 2-11                      (590,080)\n",
      "│    └─ReLU: 2-12                        --\n",
      "│    └─MaxPool2d: 2-13                   --\n",
      "├─AdaptiveAvgPool2d: 1-2                 --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─Dropout: 2-14                     --\n",
      "│    └─Linear: 2-15                      (37,752,832)\n",
      "│    └─ReLU: 2-16                        --\n",
      "│    └─Dropout: 2-17                     --\n",
      "│    └─Linear: 2-18                      (16,781,312)\n",
      "│    └─ReLU: 2-19                        --\n",
      "│    └─Linear: 2-20                      40,970\n",
      "=================================================================\n",
      "Total params: 57,044,810\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 57,003,840\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(alexnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way of modifying the pre-trained model:\n",
    "\n",
    "You may define a class and took the pre-trained network as a part of new model. If you want to build more complex model based on the pre-trained network, this might be better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "modified_alex                            --\n",
      "├─ModuleList: 1-1                        --\n",
      "│    └─Sequential: 2-1                   --\n",
      "│    │    └─Conv2d: 3-1                  (23,296)\n",
      "│    │    └─ReLU: 3-2                    --\n",
      "│    │    └─MaxPool2d: 3-3               --\n",
      "│    │    └─Conv2d: 3-4                  (307,392)\n",
      "│    │    └─ReLU: 3-5                    --\n",
      "│    │    └─MaxPool2d: 3-6               --\n",
      "│    │    └─Conv2d: 3-7                  (663,936)\n",
      "│    │    └─ReLU: 3-8                    --\n",
      "│    │    └─Conv2d: 3-9                  (884,992)\n",
      "│    │    └─ReLU: 3-10                   --\n",
      "│    │    └─Conv2d: 3-11                 (590,080)\n",
      "│    │    └─ReLU: 3-12                   --\n",
      "│    │    └─MaxPool2d: 3-13              --\n",
      "│    └─AdaptiveAvgPool2d: 2-2            --\n",
      "│    └─Sequential: 2-3                   --\n",
      "│    │    └─Dropout: 3-14                --\n",
      "│    │    └─Linear: 3-15                 (37,752,832)\n",
      "│    │    └─ReLU: 3-16                   --\n",
      "│    │    └─Dropout: 3-17                --\n",
      "│    │    └─Linear: 3-18                 (16,781,312)\n",
      "│    │    └─ReLU: 3-19                   --\n",
      "├─Linear: 1-2                            40,970\n",
      "=================================================================\n",
      "Total params: 57,044,810\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 57,003,840\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "class modified_alex(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        alexnet = torchvision.models.alexnet(weights=\"DEFAULT\")\n",
    "\n",
    "        for param in alexnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.features = nn.ModuleList(alexnet.children())\n",
    "        n_in_features = self.features[2][-1].in_features\n",
    "\n",
    "        self.features[2] = self.features[2][:-1]  # drop out the final layer\n",
    "\n",
    "        self.final = nn.Linear(n_in_features, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        z = self.final(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "alexnet2 = modified_alex()\n",
    "\n",
    "print(summary(alexnet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset. Depending on the architecture of a network, you may want to resize the input images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the training, I will use the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "# use GPU if it is available. If not, CPU will be used.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either CPU or GPU could be used, but the network and the data need to be on the same device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = alexnet.to(device)  # load the model to the GPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.223\n",
      "[1,  4000] loss: 2.187\n",
      "[1,  6000] loss: 2.095\n",
      "[1,  8000] loss: 2.178\n",
      "[1, 10000] loss: 2.170\n",
      "[1, 12000] loss: 2.277\n",
      "[2,  2000] loss: 2.060\n",
      "[2,  4000] loss: 2.126\n",
      "[2,  6000] loss: 2.193\n",
      "[2,  8000] loss: 2.291\n",
      "[2, 10000] loss: 2.192\n",
      "[2, 12000] loss: 2.260\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss_epochs = []\n",
    "train_acc_epochs = []\n",
    "\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    # running loss\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # load data to GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute the number of correct predictions\n",
    "        train_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to use custom weights saved in a file?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define network\n",
    "class simpleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear1 = nn.Linear(28 * 28, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.Linear2 = nn.Linear(512, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.Linear3 = nn.Linear(512, 512)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.Linear4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.Linear1(x))\n",
    "        x = self.relu2(self.Linear2(x))\n",
    "        x = self.relu3(self.Linear3(x))\n",
    "        z = self.Linear4(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "model = simpleNN()\n",
    "\n",
    "# save model weights. you can also save the entirle model (torch.save(model,'simple_model_with_weight.pth'))\n",
    "torch.save(model.state_dict(), \"simple_model_weight.pth\")\n",
    "\n",
    "# load model weights\n",
    "model.load_state_dict(torch.load(\"simple_model_weight.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpy2_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
